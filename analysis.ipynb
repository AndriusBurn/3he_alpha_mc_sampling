{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports\n",
    "import numpy as np, sys; sys.path.append('..')\n",
    "from matplotlib import pyplot as plt\n",
    "import corner, emcee\n",
    "from autocorr_time import integrated_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "# Code to change between runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the samples file\n",
    "file_name = 'samples.npz'\n",
    "\n",
    "# Specify the burn-in and n_steps\n",
    "n_burn = 100\n",
    "n_steps = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End code to change\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an array of the samples\n",
    "\n",
    "This block performs analysis on the sets of samples to determine if walkers were stuck, thin by the autocorrelation time, and reshapes if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the sampling method (.h5 -> emcee, .npz -> ptemcee)\n",
    "if file_name.endswith('.h5'):\n",
    "# Load in the samples\n",
    "    reader = emcee.backends.HDFBackend(file_name)\n",
    "    samples_not_flat = reader.get_chain()\n",
    "    samples_flat = reader.get_chain(flat = True)\n",
    "\n",
    "    # Look at the variances within each chain to determine if the walker is moving enough or if it is stuck.\n",
    "    within_chain_means = np.mean(samples_not_flat[n_burn:, :, :], axis = 0)\n",
    "\n",
    "    # Create an empty array of the within chain variances\n",
    "    within_chain_var = np.empty(within_chain_means.shape)\n",
    "\n",
    "    # Run a for loop across all walkers to compute the within chain variance\n",
    "    for i in range(0, within_chain_means.shape[0]):\n",
    "        within_chain_var[i, :] = np.sum(np.square(within_chain_means[i, :] - samples_not_flat[n_burn:, i, :]), axis = 0) / (samples_not_flat.shape[0] // 2)\n",
    "\n",
    "    # Get the typical within chain variance W for each parameter\n",
    "    W = np.median(within_chain_var, axis = 0)\n",
    "\n",
    "    # Now we need to loop over each chain for each parameter to see how it compares to the typical variance\n",
    "    bad_indices = []\n",
    "    ratios = np.empty(within_chain_means.shape)\n",
    "    # Loop over each parameter\n",
    "    for i in range(0, within_chain_means.shape[1]):\n",
    "        # Loop over the walkers\n",
    "        for j in range(0, within_chain_means.shape[0]):\n",
    "            ratio = np.sum(within_chain_var[j, i] / W[i]) / within_chain_means.shape[1]\n",
    "            ratios[j, i] = ratio\n",
    "\n",
    "    # Sum along each parameter, this value should be very close to 1.0. Select out the bad indices\n",
    "    total_normalized_ratios = np.sum(ratios, axis = 1)\n",
    "    bad_indices = np.where(total_normalized_ratios <= 0.9)[0]\n",
    "    print('Found {} bad walkers at indices:'.format(bad_indices.shape[0]))\n",
    "    print(bad_indices)\n",
    "\n",
    "    if bad_indices.shape[0] != 0:\n",
    "        # Remove the bad walkers\n",
    "        samples_not_flat = np.delete(samples_not_flat, bad_indices, axis = 1)\n",
    "\n",
    "    # # Thin according to the burn-in time\n",
    "    thinned_samples_not_flat = samples_not_flat[n_burn:, :, :]\n",
    "\n",
    "    # Compute the autocorrelation times for each parameter\n",
    "    ac_s = reader.get_autocorr_time(discard = n_burn, tol = 0)\n",
    "    ac = int(np.max(ac_s))\n",
    "    print('Autocorrelation time: {}'.format(ac))\n",
    "\n",
    "    # Thin according to the autocorrelation time\n",
    "    thinned_samples_not_flat = thinned_samples_not_flat[::ac, :, :]\n",
    "\n",
    "    # Flatten the samples and log-prob\n",
    "    len0, len1, len2 = thinned_samples_not_flat.shape\n",
    "    samples = np.reshape(thinned_samples_not_flat, (len0 * len1, len2))\n",
    "elif file_name.endswith('.npz'):\n",
    "    # Load in the samples\n",
    "    all_samples = np.load(file_name)['arr_0']\n",
    "\n",
    "    samples_not_flat = all_samples[0] # Just the beta = 1 samples\n",
    "    # Swap axes so it is in the shape (step, walker, parameter)\n",
    "    samples_not_flat = np.swapaxes(samples_not_flat, 0, 1)\n",
    "\n",
    "    # Look at the variances within each chain to determine if the walker is moving enough or if it is stuck.\n",
    "    within_chain_means = np.mean(samples_not_flat[:, :, :], axis = 0)\n",
    "\n",
    "    # Create an empty array of the within chain variances\n",
    "    within_chain_var = np.empty(within_chain_means.shape)\n",
    "\n",
    "    # Run a for loop across all walkers to compute the within chain variance\n",
    "    for i in range(0, within_chain_means.shape[0]):\n",
    "        within_chain_var[i, :] = np.sum(np.square(within_chain_means[i, :] - samples_not_flat[:, i, :]), axis = 0) / (samples_not_flat.shape[0] // 2)\n",
    "\n",
    "    # Get the typical within chain variance W for each parameter\n",
    "    W = np.median(within_chain_var, axis = 0)\n",
    "\n",
    "\n",
    "    # Now we need to loop over each chain for each parameter to see how it compares to the typical variance\n",
    "    bad_indices = []\n",
    "    ratios = np.empty(within_chain_means.shape)\n",
    "    # Loop over each parameter\n",
    "    for i in range(0, within_chain_means.shape[1]):\n",
    "        # Loop over the walkers\n",
    "        for j in range(0, within_chain_means.shape[0]):\n",
    "            ratio = np.sum(within_chain_var[j, i] / W[i]) / within_chain_means.shape[1]\n",
    "            ratios[j, i] = ratio\n",
    "\n",
    "    # Sum along each parameter, this value should be very close to 1.0. Select out the bad indices\n",
    "    total_normalized_ratios = np.sum(ratios, axis = 1)\n",
    "    bad_indices = np.where(total_normalized_ratios <= 0.9)[0]\n",
    "    print('Found {} bad walkers at indices:'.format(bad_indices.shape[0]))\n",
    "    print(bad_indices)\n",
    "\n",
    "    if bad_indices.shape[0] != 0:\n",
    "        # Remove the bad walkers\n",
    "        samples_not_flat = np.delete(samples_not_flat, bad_indices, axis = 1)\n",
    "\n",
    "    # Compute the autocorrelation time and thin\n",
    "    ac_s = integrated_time(samples_not_flat)\n",
    "    ac = int(np.ceil(np.max(ac_s)))\n",
    "    samples_not_flat = samples_not_flat[::ac, :, :]\n",
    "    print('Autocorrelation time: {}'.format(ac))\n",
    "\n",
    "    # Flatten the samples\n",
    "    len0, len1, len2 = samples_not_flat.shape\n",
    "    samples = np.reshape(samples_not_flat, (len0 * len1, len2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a corner plot of the samples\n",
    "labels = ['$A_{0}$', '$r_{0}$', '$(C_{1}^{+})^{2}$', '$P_{1}^{+}$', '$(C_{1}^{-})^{2}$', '$P_{1}^{-}$']\n",
    "\n",
    "corner.corner(samples[:, :6], labels = labels, quantiles = [0.16, 0.5, 0.84], show_titles = True, title_fmt = '.3f', title_kwargs = {'fontsize': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
